import subprocess
import sys
import os


def run_spark_job():
    """
    Ex√©cute le job Spark "Datacleaning.py" dans le conteneur Docker "spark-master".

    Cette fonction utilise 'subprocess.run' pour lancer une commande 'docker exec'
    qui soumet le job Spark. La commande est configur√©e pour:
    - S'ex√©cuter en tant qu'utilisateur 'root' pour √©viter les probl√®mes de permissions.
    - Utiliser le mode 'local[*]' pour le d√©veloppement sur un seul n≈ìud.
    - T√©l√©charger la d√©pendance pour Kafka ('spark-sql-kafka-0-10_2.12:3.5.0').
    - Lancer le script Python situ√© dans le volume de travail de Spark.

    En cas d'√©chec de la commande, le script s'arr√™te et renvoie un code d'erreur.
    """
    print("üî• Lancement du job Spark Datacleaning.py dans le conteneur Spark...")
    try:
        subprocess.run(
            [
                "docker",
                "exec",
                "--user",
                "root",
                "spark-master",
                "/opt/bitnami/spark/bin/spark-submit",
                "--master",
                "local[*]",
                "--conf",
                "spark.jars.ivy=/tmp/.ivy2",
                "--packages",
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0",
                "/opt/bitnami/spark/work/Datacleaning.py",
            ],
            check=True,
        )
        print("‚úÖ Le job Spark s'est termin√© avec succ√®s.")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Erreur lors de l'ex√©cution du job Spark : {e}")
        # Arr√™te le script avec un code d'erreur
        sys.exit(1)
    except FileNotFoundError:
        print(
            "‚ùå Erreur : La commande 'docker' n'a pas √©t√© trouv√©e. Assurez-vous que Docker est install√© et dans votre PATH."
        )
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Une erreur inattendue est survenue : {e}")
        sys.exit(1)


if __name__ == "__main__":
    """
    Point d'entr√©e du script.
    """
    print("‚ú® D√©marrage du script pour ex√©cuter le job Spark...")
    run_spark_job()
    print("‚û°Ô∏è Le script d'ex√©cution a termin√© son travail.")
